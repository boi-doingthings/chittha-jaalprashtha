[
  {
    "objectID": "posts/pytorch-part1/index.html",
    "href": "posts/pytorch-part1/index.html",
    "title": "PyTorch Notes-I",
    "section": "",
    "text": "Two primitives\n- `torch.utils.data.DataLoader` --> Map-Style\n- `torch.utils.data.Dataset` --> Iterable-Style\n\nMap-Style : A map-style dataset is one that implements the getitem() and len() protocols, and represents a map from (possibly non-integral) indices/keys to data samples.\n\nUseful for:\n\nRead the data values at some i-th index in the dataset. Think like pandas dataframe indexing.\n\n\nIterable-Style: An iterable-style dataset is an instance of a subclass of IterableDataset that implements the iter() protocol, and represents an iterable over data samples.\n\nUseful for:\n\nCases when random reads are expensive.\nBatch size is data dependent.\n\n\n\n\n\nCode\nimport torch\nfrom torch.utils.data import DataLoader\n\n\n\nTorch\n\nTorchVision\n\ndatasets\n\nTorchText\nTorchAudio\n\n\n\n\nCode\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n\n\n\nCode\n#collapse-on\ntrain_data = datasets.FashionMNIST(root=\"./data\", # where is data stored or to be stored\n                                   train=True, # datasets contain predefined split\n                                   download=True, #Download if not present\n                                   transform =ToTensor()\n                                  )\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\n\n\n\nCode\ntrain_data.data.shape\n\n\ntorch.Size([60000, 28, 28])\n\n\n\n\nCode\ntrain_data.classes\n\n\n['T-shirt/top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']\n\n\n\n\nCode\ntrain_data.class_to_idx\n\n\n{'T-shirt/top': 0,\n 'Trouser': 1,\n 'Pullover': 2,\n 'Dress': 3,\n 'Coat': 4,\n 'Sandal': 5,\n 'Shirt': 6,\n 'Sneaker': 7,\n 'Bag': 8,\n 'Ankle boot': 9}\n\n\n\n\nCode\ntrain_data.targets\n\n\ntensor([9, 0, 0,  ..., 3, 0, 5])\n\n\n\n\nCode\ntrain_data.transforms\n\n\nStandardTransform\nTransform: ToTensor()\n\n\n\n\nCode\ntrain_data.training_file\n\n\n'training.pt'\n\n\n\n\nCode\ntrain_data.train\n\n\nTrue\n\n\n\n\nCode\ntest_data.train\n\n\nFalse\n\n\nDataset is DataLoaderâ€™s argument.\nBenefits as it supports: 1. Automatic Batching 2. Sampling 3. Shuffling 4. Multiprocess data loading.\n\n\nCode\nbatch_size = 512\n\n# Create data loaders.\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n\n    break\n\n\nShape of X [N, C, H, W]: torch.Size([512, 1, 28, 28])\nShape of y: torch.Size([512]) torch.int64\n\n\n\n\nCode\ntest_data.data.shape\n\n\ntorch.Size([10000, 28, 28])\n\n\n\n\nCode\nlen(test_dataloader)\n\n\n20\n\n\n\n\nCode\nprint(test_data.data.shape[0]/batch_size)\n\n\n19.53125"
  },
  {
    "objectID": "posts/pytorch-part1/index.html#model-creation",
    "href": "posts/pytorch-part1/index.html#model-creation",
    "title": "PyTorch Notes-I",
    "section": "Model Creation",
    "text": "Model Creation\nIn Pytorch, a model is a class that inherits from the nn.Module. There are two main components of model class:\n\nA function __init__() : This stores the model components like: layers, dropouts etc\n\nA function forward(): The forward flow of data in the model i.e how data flows from one layer to another.\n\n\n\nCode\n# Get cpu or gpu device for training.\ndevice = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\n\nUsing cuda:1 device\n\n\n\n\nCode\nfrom torch import nn\n# Define Model\n\nclass NNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.relu = nn.ReLU()\n        self.linear1 = nn.Linear(28*28,512)\n        self.linear2 = nn.Linear(512,256*2)\n        self.linear3 = nn.Linear(256*2,10)\n        \n    def forward(self,x):\n        '''\n        x : denotes the input data that will flow through the model\n        '''\n        x = self.flatten(x)\n        x = self.relu(self.linear1(x))\n        x = self.relu(self.linear2(x))\n        return self.linear3(x)\n    \nmodel = NNet().to(device)\nprint(model)\n        \n        \n        \n    \n\n\nNNet(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (relu): ReLU()\n  (linear1): Linear(in_features=784, out_features=512, bias=True)\n  (linear2): Linear(in_features=512, out_features=512, bias=True)\n  (linear3): Linear(in_features=512, out_features=10, bias=True)\n)\n\n\n\nLoss\nThe metric indicator of the scale of how right or wrong our predictions are against the actual distributions. ### Optimizer The process that update the weights to minimize the loss.\n\n\nCode\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n\n\n\nCode\nlen(train_dataloader)\n\n\n118\n\n\n\n\nCode\ndef train(dataloader,model,loss_fn,optimizer):\n    # You can directly apply the len function on dataloader.dataset to get the number of entries in the dataset.\n    # Doing this on the dataloader returns the number of iteables formed\n    size = len(dataloader.dataset) \n    model.train() #model in training mode\n    for batch, (X,y) in enumerate(dataloader):\n        X,y = X.to(device), y.to(device)\n        \n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad() # clear the gradients and set to zero\n        loss.backward() # do backprop\n        optimizer.step() # udpate the gradients\n        \n        if batch % 50 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n        \n    \n\n\n\n\nCode\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval() #model in eval mode so no gradient updates\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n\n\n\n\nCode\n%%time\nepochs = 8\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n\n\nEpoch 1\n-------------------------------\nloss: 2.294676  [    0/60000]\nloss: 2.288279  [25600/60000]\nloss: 2.275253  [51200/60000]\nTest Error: \n Accuracy: 23.6%, Avg loss: 2.275554 \n\nEpoch 2\n-------------------------------\nloss: 2.277005  [    0/60000]\nloss: 2.270516  [25600/60000]\nloss: 2.256798  [51200/60000]\nTest Error: \n Accuracy: 29.2%, Avg loss: 2.257794 \n\nEpoch 3\n-------------------------------\nloss: 2.259315  [    0/60000]\nloss: 2.252728  [25600/60000]\nloss: 2.238159  [51200/60000]\nTest Error: \n Accuracy: 32.6%, Avg loss: 2.239789 \n\nEpoch 4\n-------------------------------\nloss: 2.241425  [    0/60000]\nloss: 2.234566  [25600/60000]\nloss: 2.219000  [51200/60000]\nTest Error: \n Accuracy: 36.0%, Avg loss: 2.221168 \n\nEpoch 5\n-------------------------------\nloss: 2.222970  [    0/60000]\nloss: 2.215658  [25600/60000]\nloss: 2.198995  [51200/60000]\nTest Error: \n Accuracy: 40.5%, Avg loss: 2.201538 \n\nEpoch 6\n-------------------------------\nloss: 2.203543  [    0/60000]\nloss: 2.195568  [25600/60000]\nloss: 2.177734  [51200/60000]\nTest Error: \n Accuracy: 45.4%, Avg loss: 2.180485 \n\nEpoch 7\n-------------------------------\nloss: 2.182732  [    0/60000]\nloss: 2.173880  [25600/60000]\nloss: 2.154761  [51200/60000]\nTest Error: \n Accuracy: 49.2%, Avg loss: 2.157631 \n\nEpoch 8\n-------------------------------\nloss: 2.160224  [    0/60000]\nloss: 2.150239  [25600/60000]\nloss: 2.129723  [51200/60000]\nTest Error: \n Accuracy: 51.6%, Avg loss: 2.132602 \n\nDone!\nCPU times: user 24.9 s, sys: 184 ms, total: 25 s\nWall time: 25.2 s"
  },
  {
    "objectID": "posts/places_visit/index.html",
    "href": "posts/places_visit/index.html",
    "title": "My Travels",
    "section": "",
    "text": "I am an avid traveller and is always up for the next trip with my friends.\n\n\n\n\n\n\nKurukshetra\n\nNational Institute of Technology\nBrahma Sarovar\nJyotisar\nKrishna Museum\nPanorama & Science Centre\nSheikh Chili Masoleum\nKalpana Chawla Memorial Planetorium\nKurukshetra University\n\nKarnal\nAmbala\nPanchkula\nPanipat\n\n\n\n\n\nSolan -> Kasauli\n\nMonkey Point\nGilbert Trail\nMonkey (Manki) Point\nMall Road\n\nShimla\n\nNarkanda -> Hatu Peak -> Hatu Mata Temple\nNarkanda -> Tannu Jubar Lake\nMall Road\n\nKullu\n\nKasol\nBhuntar\nManikaran Sahib and Hot Springs\nTosh\nParvati Valley\nChalal\n\nKangra -> Dharamshala\n\nTriund\nMoon Peak\nMcLeodganj\nBhagsunag Falls\n\n\n\n\n\n\nJaipur\nUdaipur\nKota\n\n\n\n\n\nNorth Goa\n\nBeaches\n\nArambol\nBaga\nCalangute\nCandolim\n\n\nSouth Goa\n\nBeaches\n\nPalolem (Fav)\nAgonda\nMorjim\n\n\n\n\n\n\n\nMumbai\n\nMarine Drive\nGateway of India\nDalal Street\nBandra-Worli Sealink\nBandra\nCST\nNariman Point\nTravel in Mumbai Local\n\nPune\nLonavala\n\n\n\n\n\nTirupati\n\nSri Venkateshwara Temple (Balaji)\n\n\n\n\n\n\nMathura\n\nEverywhere :)\n\nAgra\n\nTaj Mahal\nAgra Fort\nAkbar Tomb\n\n\n\n\n\n\nIndore\n\n56 Food Street\nSarafa Bazar (Over-Rated)\nRalamandal Wildlife Sanctuary\n\nUjjain\n\nMahakaleshwar Jyotirlinga\nBhairo Baba Temple\n\n\n\n\n\n\nAlmost Everywhere :)\n\n\n\n\n\nUttar Kannada\n\nGokarna\n\nOm Beach\nKudle Beach\nMahabaleshwar Temple\n\nBhatkal\n\nMurudeshwar Temple\n\n\nBengaluru\n\nTraffic :|\nIndian Institute of Science (IISc)\nCubbon Park\nVisvesvaraya Industrial & Technological Museum\nLal Bagh Botanical Garden\nCity Market"
  },
  {
    "objectID": "posts/sample-test/index.html",
    "href": "posts/sample-test/index.html",
    "title": "Quarto Document",
    "section": "",
    "text": "Red\nGreen\nBlue"
  },
  {
    "objectID": "posts/sample-test/index.html#shapes",
    "href": "posts/sample-test/index.html#shapes",
    "title": "Quarto Document",
    "section": "2 Shapes",
    "text": "2 Shapes\n\nSquare\nCircle\nTriangle"
  },
  {
    "objectID": "posts/sample-test/index.html#textures",
    "href": "posts/sample-test/index.html#textures",
    "title": "Quarto Document",
    "section": "3 Textures",
    "text": "3 Textures\n\nSmooth\nBumpy\nFuzzy"
  },
  {
    "objectID": "posts/sample-test/index.html#overview",
    "href": "posts/sample-test/index.html#overview",
    "title": "Quarto Document",
    "section": "4 Overview",
    "text": "4 Overview\nKnuth says always be literate [@knuth1984].\n\n\nCode\n1 + 1\n\n\n2"
  },
  {
    "objectID": "posts/sample-test/index.html#overview1",
    "href": "posts/sample-test/index.html#overview1",
    "title": "Quarto Document",
    "section": "5 Overview1",
    "text": "5 Overview1\nSee FigureÂ 1 in SectionÂ 6 for a demonstration of a simple plot.\nSee EquationÂ 1 to better understand standard deviation."
  },
  {
    "objectID": "posts/sample-test/index.html#sec-plot",
    "href": "posts/sample-test/index.html#sec-plot",
    "title": "Quarto Document",
    "section": "6 Plot",
    "text": "6 Plot\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n\n\n\n\nFigureÂ 1: Simple Plot"
  },
  {
    "objectID": "posts/sample-test/index.html#sec-equation",
    "href": "posts/sample-test/index.html#sec-equation",
    "title": "Quarto Document",
    "section": "7 Equation",
    "text": "7 Equation\n\ns = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}\n\\tag{1}"
  },
  {
    "objectID": "posts/sample-test/index.html#placing-colorbars",
    "href": "posts/sample-test/index.html#placing-colorbars",
    "title": "Quarto Document",
    "section": "8 Placing Colorbars",
    "text": "8 Placing Colorbars\nColorbars indicate the quantitative extent of image data. Placing in a figure is non-trivial because room needs to be made for them. The simplest case is just attaching a colorbar to each axes:1.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axs = plt.subplots(2, 2)\nfig.set_size_inches(20, 8)\ncmaps = ['RdBu_r', 'viridis']\nfor col in range(2):\n    for row in range(2):\n        ax = axs[row, col]\n        pcm = ax.pcolormesh(\n          np.random.random((20, 20)) * (col + 1),\n          cmap=cmaps[col]\n        )\n        fig.colorbar(pcm, ax=ax)\nplt.show()"
  },
  {
    "objectID": "posts/sample-test/index.html#references",
    "href": "posts/sample-test/index.html#references",
    "title": "Quarto Document",
    "section": "9 References",
    "text": "9 References"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I am Yash Gupta aka boi-doingthings. I am interested in using Data Science to solve business problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Boi-DoingThings Blog",
    "section": "",
    "text": "PyTorch Notes-I\n\n\n\n\n\n\n\njupyter\n\n\npytorch\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nQuarto Document\n\n\n\n\n\n\n\n\n\n\n\n\nBoi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Travels\n\n\n\n\n\n\n\ntravel\n\n\n\n\nA list of places I have been.\n\n\n\n\n\n\nSep 17, 2022\n\n\n\n\n\n\nNo matching items"
  }
]